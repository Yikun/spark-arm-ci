name: Spark Test on ARM

on:
  pull_request:
    branches:
      - main
  schedule:
    - cron:  '0 9,17 * * *'
env:
  ACCESS_KEY_ID: ${{ secrets.ACCESS_KEY_ID }}
  SECRET_ACCESS_KEY: ${{ secrets.SECRET_ACCESS_KEY }}
  REGION: "ap-southeast-3"
  PROJECT_ID: "c847fb717563484fa95d18a75ae7831c"
  CLUSTER_ID: ${{ secrets.CLUSTER_ID }}

jobs:
  build-arm:
    name: "Spark on ARM64: ${{ matrix.comment }}"
    strategy:
      matrix:
        include:
          - type: python
            comment: "PySpark build and test"
          - type: build
            comment: "Maven build"
    runs-on: ubuntu-20.04
    steps:
      - name: Checkout
        uses: actions/checkout@v2
      - name: Get kubectl
        run: |
          curl -sL https://dl.k8s.io/v1.19.0/kubernetes-client-linux-amd64.tar.gz | tar zx -C $HOME/
          export PATH=PATH:$HOME/kubernetes/client/bin
      - name: Get ARM cluster credetials
        uses: huaweicloud/cce-cluster-credentials@v1
        with:
          region: "${{ env.REGION }}"
          access-key-id: "${{ env.ACCESS_KEY_ID }}"
          access-key-secret: "${{ env.SECRET_ACCESS_KEY }}"
          project-id: "${{ env.PROJECT_ID }}"
          cluster-id: "${{ env.CLUSTER_ID }}"
      - name: Run build and test job
        run: |
          kubectl config use-context external
          POD_NAME=spark-`date '+%Y-%m-%d-%H%M%y-%s'`-$RANDOM
          kubectl run --rm -i --image yikunkero/spark-multiarch:latest --image-pull-policy=Always \
                  --requests "cpu=4,memory=8Gi" \
                  $POD_NAME --restart=Never \
                  --timeout 3m \
                  ${{ matrix.type }}
          STATUS=$?
          echo "Status: $STATUS"
          kubectl delete -f $POD_NAME || true
          if [[ $STATUS -ne 0 ]]; then echo "Job failed..."; /bin/false; fi

  spark-arm:
    if: always()
    name: "Spark on ARM64: ${{ matrix.modules }} ${{ matrix.comment }} (JDK ${{ matrix.java }}, ${{ matrix.hadoop }}, ${{ matrix.hive }})"
    runs-on: ubuntu-20.04
    strategy:
      fail-fast: false
      max-parallel: 3
      matrix:
        java:
          - 8
        hadoop:
          - hadoop3
        hive:
          - hive2.3
        # TODO(SPARK-32246): We don't test 'streaming-kinesis-asl' for now.
        # Kinesis tests depends on external Amazon kinesis service.
        # Note that the modules below are from sparktestsupport/modules.py.
        modules:
          - >-
            core, unsafe, kvstore, avro,
            network-common, network-shuffle, repl, launcher,
            examples, sketch, graphx
          - >-
            catalyst, hive-thriftserver
          - >-
            streaming, sql-kafka-0-10, streaming-kafka-0-10,
            mllib-local, mllib,
            yarn, mesos, kubernetes, hadoop-cloud, spark-ganglia-lgpl
        # Here, we split Hive and SQL tests into some of slow ones and the rest of them.
        included-tags: [""]
        excluded-tags: [""]
        comment: [""]
        include:
        # Hive tests
          - modules: hive
            java: 8
            hadoop: hadoop3
            hive: hive2.3
            included-tags: org.apache.spark.tags.SlowHiveTest
            comment: "- slow tests"
          - modules: hive
            java: 8
            hadoop: hadoop3
            hive: hive2.3
            excluded-tags: org.apache.spark.tags.SlowHiveTest
            comment: "- other tests"
          # SQL tests
          - modules: sql
            java: 8
            hadoop: hadoop3
            hive: hive2.3
            included-tags: org.apache.spark.tags.ExtendedSQLTest
            comment: "- slow tests"
          - modules: sql
            java: 8
            hadoop: hadoop3
            hive: hive2.3
            excluded-tags: org.apache.spark.tags.ExtendedSQLTest
            comment: "- other tests"
    steps:
      - name: Checkout
        uses: actions/checkout@v2
      - name: Get kubectl
        run: |
          curl -sL https://dl.k8s.io/v1.19.0/kubernetes-client-linux-amd64.tar.gz | tar zx -C $HOME/
          export PATH=PATH:$HOME/kubernetes/client/bin
      - name: Get ARM cluster credetials
        uses: huaweicloud/cce-cluster-credentials@v1
        with:
          region: "${{ env.REGION }}"
          access-key-id: "${{ env.ACCESS_KEY_ID }}"
          access-key-secret: "${{ env.SECRET_ACCESS_KEY }}"
          project-id: "${{ env.PROJECT_ID }}"
          cluster-id: "${{ env.CLUSTER_ID }}"
      - name: Run build and test job
        run: |
          kubectl config use-context external
          POD_NAME=spark-`date '+%Y-%m-%d-%H%M%y-%s'`-$RANDOM
          kubectl run --rm -i --image yikunkero/spark-multiarch:latest --image-pull-policy=Always \
                  --requests "cpu=4,memory=8Gi" \
                  $POD_NAME --restart=Never \
                  --env="MODULES_TO_TEST=${{ matrix.modules }}" \
                  --env="EXCLUDED_TAGS=${{ matrix.excluded-tags }}" \
                  --env="INCLUDED_TAGS=${{ matrix.included-tags }}" \
                  --env="HADOOP_PROFILE=${{ matrix.hadoop }}" \
                  --env="HIVE_PROFILE=${{ matrix.hive }}" \
                  --env="SPARK_LOCAL_IP=${{ matrix.hive }}" \
                  --env="SPARK_LOCAL_IP=localhost" \
                  --timeout 3m \
                  scala
          STATUS=$?
          echo "Status: $STATUS"
          kubectl delete -f $POD_NAME || true
          if [[ $STATUS -ne 0 ]]; then echo "Job failed..."; /bin/false; fi
